<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding">
  <meta name="keywords" content="OphNet, surgical-workflow-understanding, ophthalmic-surgery">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/minghu0830">Ming Hu</a><sup>1,2,3 *</sup>,</span>
            <span class="author-block">
              <a href="https://richard-peng-xia.github.io/">Peng Xia</a><sup>1,3 *</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=A_mtOiMAAAAJ&hl=en">Lin Wang</a><sup>5 *</sup>,</span>
            <span class="author-block">
              <a href="https://siyuanyan1.github.io/">Siyuan Yan</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Z5BHRAYAAAAJ&hl=en">Feilong Tang</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Og0Ewb8AAAAJ&hl=en">Zhongxing Xu</a><sup>7</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=InHF3ykAAAAJ">Yimin Luo</a><sup>6</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=BUF_-N8AAAAJ&hl=en">Kaimin Song</a><sup>3</sup>,</span>   
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=mzZy2NMAAAAJ&hl=en">Jurgen Leitner</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://xueliancheng.github.io/">Xuelian Cheng</a><sup>1</sup>,</span>
              <span class="author-block">
            <span class="author-block">
              <a href="https://samjcheng.github.io/">Jun Cheng</a><sup>8</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=OVA8Q54AAAAJ&hl=en">Chi Liu</a><sup>9</sup>,</span>
            <span class="author-block">
              <a href="">Kaijing Zhou</a><sup>4 †</sup>,</span>
            <span class="author-block">
              <a href="https://zongyuange.github.io/">Zongyuan Ge</a><sup>1,2,3 †</sup></span> 
            </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>AIM Lab, Faculty of IT, Monash University,</span>
            <span class="author-block"><sup>2</sup>Faculty of Engineering, Monash University</span>
            <span class="author-block"><sup>3</sup>Airdoc-Monash Research, Airdoc,</span>
            <span class="author-block"><sup>4</sup>Eye Hospital, Wenzhou Medical University,</span>
            <span class="author-block"><sup>5</sup>Bosch Corporate Research</span>
            <span class="author-block"><sup>6</sup>King's College London,</span>
            <span class="author-block"><sup>7</sup>Cornell University,</span>
            <span class="author-block"><sup>8</sup>Institute for Infocomm Research, A*STAR</span>
            <span class="author-block"><sup>9</sup>Faculty of Data Science, City University of Macau</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2406.07471"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/minghu0830/OphNet-benchmark"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/minghu0830/OphNet-benchmark"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/images/1.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">OphNet:</span>the largest video dataset for ophthalmic surgical workflow analysis
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Surgical scene perception via videos are critical for advancing robotic surgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology. However, the scarcity of diverse and richly annotated video datasets has hindered the development of intelligent systems for surgical workflow analysis. Existing datasets for surgical workflow analysis, which typically face challenges such as small scale, a lack of diversity in surgery and phase categories, and the absence of time-localized annotations, limit the requirements for action understanding and model generalization validation in complex and diverse real-world surgical scenarios. To address this gap, we introduce OphNet, a large-scale, expert-annotated video benchmark for ophthalmic surgical workflow understanding.
          </p>
          <p>
            OphNet features: 
          </p>
          <p>
            1) A diverse collection of 2,278 surgical videos spanning 66 types of cataract, glaucoma, and corneal surgeries, with detailed annotations for 102 unique surgical phases and 150 fine-grained operations; 
          </p>
          <p>
            2) It offers sequential and hierarchical annotations for each surgery, phase, and operation, enabling comprehensive understanding and improved interpretability; 
          </p> 
          <p>
            3) Moreover, OphNet provides time-localized annotations, facilitating temporal localization and prediction tasks within surgical workflows. With approximately 205 hours of surgical videos, OphNet is about 20 times larger than the largest existing surgical workflow analysis benchmark.
          </p>
        </div>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <!-- Re-rendering. -->
    <div class="column">
      <!-- <h3 class="title is-4">Operation</h3> -->
      <div class="content has-text-justified">
        <p>

        </p>
      </div>
      <div class="content has-text-centered">
        <img id="replay-image" src="./static/images/loca.png" alt="Re-rendering the input video" width="75%">
      </div>
    </div>

    <div class="container is-max-desktop">
      <!-- Re-rendering. -->
      <div class="column">
        <!-- <h3 class="title is-4">Operation</h3> -->
        <div class="content has-text-justified">
          <p>
  
          </p>
        </div>
        <div class="content has-text-centered">
          <img id="replay-image" src="./static/images/sta.png" alt="Re-rendering the input video" width="75%">
        </div>
      </div>

</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Re-rendering. -->
    <div class="column">
      <h3 class="title is-4">Operation</h3>
      <div class="content has-text-justified">
        <p>
          xxxx
        </p>
      </div>
      <div class="content has-text-centered">
        <img id="replay-image" src="./static/images/2.jpg" alt="Re-rendering the input video" width="75%">
      </div>
    </div>

    <!--/ Re-rendering. -->
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-4">Phase</h2>
          <p>
           xxx
          </p>
          <img id="dollyzoom" src="./static/images/2.jpg" alt="Dolly Zoom" height="100%">
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-4">Surgery</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              xxx
            </p>
            <img id="matting-image" src="./static/images/3.png" alt="Matting" height="100%">
          </div>
        </div>
      </div>
    </div>
    <!--/ Matting. -->




    
  </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{hu2024ophnetlargescalevideobenchmark,
      title={OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding}, 
      author={Ming Hu and Peng Xia and Lin Wang and Siyuan Yan and Feilong Tang and Zhongxing Xu and Yimin Luo and Kaimin Song and Jurgen Leitner and Xuelian Cheng and Jun Cheng and Chi Liu and Kaijing Zhou and Zongyuan Ge},
      year={2024},
      eprint={2406.07471},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.07471}, 
}</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is developed based on <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies.</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
